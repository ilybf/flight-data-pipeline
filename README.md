<div align="center">

# âœˆï¸ Flight Data Warehouse Project

[![Docker](https://img.shields.io/badge/Docker-20.10%2B-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-4169E1?style=for-the-badge&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.0%2B-017CEE?style=for-the-badge&logo=apache-airflow&logoColor=white)](https://airflow.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.8%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Talend](https://img.shields.io/badge/Talend-8.0.1-FF5B00?style=for-the-badge&logo=talend&logoColor=white)](https://www.talend.com/)

**A comprehensive data engineering project implementing a modern data warehouse pipeline for flight analytics.**

[Quick Start](#ğŸš€-quick-start) â€¢ [Architecture](#ğŸ“Š-architecture) â€¢ [Data-Model](#ğŸ—„ï¸-data-model) â€¢ [Performance](#âš¡-performance-optimizations)

</div>

---

## ğŸš€ Project Overview

This project demonstrates an **end-to-end ETL pipeline** processing raw flight and booking data through **bronze**, **silver**, and **gold** layers, culminating in an analytical **Star Schema** for business intelligence and reporting. Data enrichment is performed by integrating a **Weather API**.

## ğŸ“Š Architecture

### Data Flow

```mermaid
flowchart LR
Â  Â  subgraph "Source"
Â  Â  Â  Â  Raw[(Local Files/APIs<br/>Flight/Booking Data)]
Â  Â  Â  Â  WeatherAPI[(External Weather API<br/>Real-time Data)]
Â  Â  end
Â  Â Â 
Â  Â  subgraph "ETL/Processing"
Â  Â  Â  Â  Python[Python ETL Script<br/>Multi-threaded Ingestion]
Â  Â  Â  Â  Talend[Talend 8.0.1<br/>Data Transformation/Cleansing]
Â  Â  Â  Â  Airflow[Apache Airflow 2.0+<br/>Workflow Orchestrator]
Â  Â  end
Â  Â Â 
Â  Â  subgraph "Storage"
Â  Â  Â  Â  DBeaver(PostgreSQL DB<br/>Docker Container)
Â  Â  Â  Â  Bronze[Bronze Layer<br/>Raw Data]
Â  Â  Â  Â  Silver[Silver Layer<br/>Cleaned/Staging Data]
Â  Â  Â  Â  Gold[Gold Layer<br/>Star Schema]
Â  Â  end
Â  Â Â 
Â  Â  Raw -->|Extract| Python
Â  Â  Python -->|Load| DBeaver
Â  Â  DBeaver -.Bronze-> Talend
Â  Â  Talend -->|Transform & Load| Silver
Â  Â  Silver -->|Dimensional Model| Talend
Â  Â  Talend -->|Enrichment| WeatherAPI
Â  Â  Talend -->|Load| Gold
Â  Â  Airflow -.orchestrates.-> Python
Â  Â  Airflow -.orchestrates.-> Talend

Â  Â  style DBeaver fill:#4169E1,stroke:#4169E1,color:#fff,stroke-width:2px
Â  Â  style Python fill:#3776AB,stroke:#3776AB,color:#fff,stroke-width:2px
Â  Â  style Talend fill:#FF5B00,stroke:#FF5B00,color:#fff,stroke-width:2px
Â  Â  style Airflow fill:#017CEE,stroke:#017CEE,color:#fff,stroke-width:2px
```

### Technology Stack

- **Database & Procedures**: PostgreSQL with DBeaver
- **Data Ingestion**: Python (multi-threading, yield buffers)
- **Data Transformation**: Talend 8.0.1 (pagination)
- **Orchestration**: Apache Airflow
- **Data Modeling**: Star Schema
- **Data Enrichment**: Weather API

## ğŸ“Š Data Model

### Dimensions

- `DIM_PASSENGER` - Passenger demographics and loyalty status
- `DIM_FLIGHT` - Flight details and status (6M+ records)
- `DIM_AIRPORT` - Airport geographical information
- `DIM_DATE` - Time intelligence for analysis
- `DIM_TICKET` - Booking class and fare information
- `DIM_PAYMENT` - Payment methods and status
- `DIM_WEATHER` - Weather conditions at flight time (API-enriched)

### Fact Table

- `FACT_BOOKING` - Core business metrics (500K+ records)

![Architecture Diagram](images/tables.png)

---

## ğŸ“¦ Data Sources & Volume

The pipeline ingests data from two primary sources: a large historical file for flight details and a stream of booking records, enriched with external real-time weather data.

| Data Source      | Volume/Type               | Description                                                    | Format  |
| ---------------- | ------------------------- | -------------------------------------------------------------- | ------- |
| **Flight Data**  | 6M+ Historical Records    | Static dataset for `DIM_FLIGHT` and `DIM_AIRPORT`              | CSV     |
| **Booking Data** | 500K+ Incremental Records | Core transaction data for `FACT_BOOKING` and `DIM_PASSENGER`   | CSV/API |
| **Weather API**  | Real-time                 | Enriched weather conditions linked to flight departure/arrival | JSON    |

---

## âš¡ Performance Optimizations

### Python ETL

- **Multi-threading** for parallel data processing
- **Yield with buffers** for memory-efficient chunk processing
- **Batch processing** of 6M+ records in DIM_FLIGHT

### Talend Transformation

- **Pagination implementation** for large dataset handling
- **Efficient data cleansing** before Silver/Gold layers
- **Direct loading** to both Silver and Gold layers

- **Talend Parent Job Zip** : https://drive.google.com/file/d/1nBHI3AoUCiO2dknfszf8PRDOlRK9x1Pk/view?usp=sharing

### Airflow Orchestration

- **Daily scheduled workflows**
- **Dependency management** between components
- **Error handling and monitoring**

## ğŸ› ï¸ Installation & Setup

# Required

- Python 3.8+
- Apache Airflow 2.0+
- Talend 8.0.1 (Desktop Application)
- Docker Engine 20.10+ (for PostgreSQL DB)
- DBeaver (or any PostgreSQL client)

### Quick Start

```bash
docker compose up --build -d

# Wait for services to be healthy (2-3 minutes)
docker compose ps

```

### Access Web UIs

| Service              | URL                   | Credentials   |
| -------------------- | --------------------- | ------------- |
| **Airflow**          | http://localhost:8082 | admin / admin |
| **Spark Master**     | http://localhost:8080 | -             |
| **HDFS NameNode**    | http://localhost:9870 | -             |
| **Jupyter Notebook** | http://localhost:8888 | -             |

## ğŸ“ File Structure

```
ETL-FLIGHT/
â”œâ”€â”€ airflow
â”‚   â”œâ”€â”€ config
â”‚   â”œâ”€â”€ dags
â”‚   â”œâ”€â”€ logs
â”‚   â””â”€â”€ plugins
â”œâ”€â”€ config
â”œâ”€â”€ dags
â”œâ”€â”€ data
â”œâ”€â”€ docker-compose.env
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Dockerfile.kafka-connect
â”œâ”€â”€ drivers
â”‚   â””â”€â”€ postgresql-42.7.7.jar
â”œâ”€â”€ etl-flight
â”‚   â”œâ”€â”€ airlines_modified.csv
â”‚   â”œâ”€â”€ airport.csv
â”‚   â”œâ”€â”€ api.py
â”‚   â”œâ”€â”€ bronze_layer_workflows.py
â”‚   â”œâ”€â”€ common.py
â”‚   â”œâ”€â”€ constants.py
â”‚   â”œâ”€â”€ flights_airlines.csv
â”‚   â”œâ”€â”€ flights_modified.csv
â”‚   â”œâ”€â”€ jobInfo.properties
â”‚   â”œâ”€â”€ lib
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ parent_job
â”‚   â”œâ”€â”€ __pycache__
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ etl_scripts
â”œâ”€â”€ flights_airlines.csv
â”œâ”€â”€ flights_modified.csv
â”œâ”€â”€ images
â”‚   â”œâ”€â”€ architecture.png
â”‚   â””â”€â”€ tables.png
â”œâ”€â”€ jobInfo.properties
â”œâ”€â”€ kafka_scripts
â”‚   â”œâ”€â”€ hsperfdata_root
â”‚   â”œâ”€â”€ kafka_consumer.py
â”‚   â”œâ”€â”€ kafka_consumer_stream.py
â”‚   â”œâ”€â”€ kafka_streaming_job.py
â”‚   â”œâ”€â”€ spark-5abf7a35-4957-412d-b3a2-30ff589f0315
â”‚   â”œâ”€â”€ spark-659f5745-06a0-4fdd-b0c4-38537ee2a0b9
â”œâ”€â”€ logs
â”‚   â””â”€â”€ scheduler
â”œâ”€â”€ myenv
â”‚   â”œâ”€â”€ bin
â”‚   â”œâ”€â”€ include
â”‚   â”œâ”€â”€ lib
â”‚   â”œâ”€â”€ lib64 -> lib
â”‚   â””â”€â”€ pyvenv.cfg
â”œâ”€â”€ notebooks
â”‚   â”œâ”€â”€ producer.py
â”‚   â”œâ”€â”€ sample_spark.ipynb
â”‚   â””â”€â”€ spark_local_temp
â”œâ”€â”€ parent_job
â”‚   â”œâ”€â”€ airport_dim_0_1.jar
â”‚   â”œâ”€â”€ booking_fact_0_1.jar
â”‚   â”œâ”€â”€ date_dim_0_1.jar
â”‚   â”œâ”€â”€ depi
â”‚   â”œâ”€â”€ flight_dim_0_1.jar
â”‚   â”œâ”€â”€ items
â”‚   â”œâ”€â”€ log4j2.xml
â”‚   â”œâ”€â”€ parent_job_0_1.jar
â”‚   â”œâ”€â”€ parent_job_run.bat
â”‚   â”œâ”€â”€ parent_job_run.ps1
â”‚   â”œâ”€â”€ parent_job_run.sh
â”‚   â”œâ”€â”€ passenger_dim_0_1.jar
â”‚   â”œâ”€â”€ payment_dim_0_1.jar
â”‚   â”œâ”€â”€ src
â”‚   â”œâ”€â”€ ticket_dim_0_1.jar
â”‚   â”œâ”€â”€ weather_dim_0_1.jar
â”‚   â””â”€â”€ xmlMappings
â”œâ”€â”€ parent_job_localhost.zip
â”œâ”€â”€ parent_job_worker.zip
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ hadoop
â”‚   â””â”€â”€ pyspark
â”œâ”€â”€ spark_drivers
â”œâ”€â”€ talend_jobs
```

---

## ğŸ”„ Airflow DAG: ETL-Flight

The main DAG orchestrates the complete ETL pipeline with 12 tasks:

```mermaid
flowchart TB
Â  Â  subGraph "Setup & Ingestion (Bronze)"
Â  Â  Â  Â  A[wait_for_postgres<br/>DB Health Check] --> B[python_load_flights<br/>Ingest DIM_FLIGHT (6M+)]
Â  Â  Â  Â  A --> C[python_load_bookings<br/>Ingest FACT_BOOKING (500K+)]
Â  Â  end
Â  Â Â 
Â  Â  subGraph "Transformation (Silver & Gold)"
Â  Â  Â  Â  B --> D[talend_clean_silver<br/>Cleanse & Stage Data]
Â  Â  Â  Â  C --> D
Â  Â  Â  Â  D --> E[talend_dim_build<br/>Build Dimensions]
Â  Â  Â  Â  E --> F[talend_fact_load<br/>Load FACT_BOOKING]
Â  Â  Â  Â  F --> G[weather_api_enrich<br/>Enrich DIM_WEATHER]
Â  Â  end
Â  Â Â 
Â  Â  subGraph "Validation & Finish"
Â  Â  Â  Â  G --> H[validate_gold_schema<br/>Check Star Schema]
Â  Â  Â  Â  H --> I[pipeline_complete<br/>âœ… Done]
Â  Â  end

Â  Â  style A fill:#4FC3F7,stroke:#0288D1,color:#000,stroke-width:2px
Â  Â  style B fill:#3776AB,stroke:#1D4B7D,color:#fff,stroke-width:3px
Â  Â  style C fill:#3776AB,stroke:#1D4B7D,color:#fff,stroke-width:3px
Â  Â  style D fill:#FF7043,stroke:#D84315,color:#fff,stroke-width:3px
Â  Â  style E fill:#FFD54F,stroke:#F57C00,color:#000,stroke-width:2px
Â  Â  style F fill:#FFD54F,stroke:#F57C00,color:#000,stroke-width:2px
Â  Â  style G fill:#BA68C8,stroke:#7B1FA2,color:#fff,stroke-width:2px
Â  Â  style H fill:#81C784,stroke:#388E3C,color:#000,stroke-width:2px
Â  Â  style I fill:#66BB6A,stroke:#2E7D32,color:#fff,stroke-width:3px
```

**Pipeline Duration**: ~4-6 minutes  
**Critical Path**: Spark ETL job takes ~4 minutes

---

## ğŸ› ï¸ Technology Stack

### Data Technologies

| Technology       | Version | Role                | Why Chosen                                                              |
| ---------------- | ------- | ------------------- | ----------------------------------------------------------------------- |
| **Apache Spark** | 3.3.0   | ETL Engine          | Industry standard for big data processing, supports batch and streaming |
| **Apache Hive**  | 2.3.2   | Data Warehouse      | SQL interface, Parquet storage, integrates with Hadoop ecosystem        |
| **HDFS**         | 3.3.5   | Distributed Storage | Fault-tolerant, scalable storage for big data                           |
| **SQL Server**   | 2022    | Source Database     | Enterprise-grade RDBMS, supports advanced features                      |

### Orchestration & Infrastructure

| Technology         | Version | Role                   | Why Chosen                                        |
| ------------------ | ------- | ---------------------- | ------------------------------------------------- |
| **Apache Airflow** | 2.7.3   | Workflow Orchestration | Programmable workflows, rich UI, strong community |
| **Docker**         | 20.10+  | Containerization       | Consistent environments, easy deployment          |
| **PostgreSQL**     | 13      | Airflow Metadata       | Reliable metadata store for Airflow               |
| **Redis**          | Latest  | Message Broker         | Fast Celery executor backend for Airflow          |

## âœ¨ Features âœˆï¸

### Core Capabilities

**ğŸ“Š Complete ETL Pipeline: Automated data movement from local files/APIs through Bronze, Silver, and Gold layers in PostgreSQL**

**ğŸ”„ Workflow Orchestration: Apache Airflow manages complex, multi-stage DAGs with dependencies between Python ingestion and Talend transformation jobs.**

**âš¡ High-Volume Ingestion: Python scripts use multi-threading and yield buffers for memory-efficient and fast loading of large datasets (e.g., 6M+ flight records).**

**ğŸ§© Complex Transformation: Talend 8.0.1 handles data cleansing, standardization, and dimensional modeling, including pagination for large data flows.**

**ğŸ¯ Star Schema: Optimized dimensional model with 7 Dimension Tables and 1 Fact Table for high-performance flight analytics.**

**â˜ï¸ Data Enrichment: Integrated Weather API to enrich the DIM_WEATHER table, providing context for flight delays and operational analysis.**

**ğŸ³ Containerized Database: The entire data warehouse is hosted on a Dockerized PostgreSQL instance for a stable and easily reproducible environment.**

**ğŸ”Œ Direct Connectivity: JDBC/ODBC connectivity between Talend and the PostgreSQL data warehouse for efficient data manipulation.**

---

## ğŸ”§ Troubleshooting

### Common Issues & Solutions

<details>
<summary><strong>ğŸ”´ Airflow tasks stuck in "queued"</strong></summary>

**Symptoms**: Tasks remain in "queued" state indefinitely

**Root Cause**: Redis not on same Docker network as Airflow

**Solution**:

```yaml
# In docker-compose.yml, ensure Redis has:
redis:
  networks:
    - kafka-net # Add this network
```

**Verify**:

```bash
docker exec etl-flight-airflow-worker-1 redis-cli -h redis ping
# Should return: PONG
```

</details>
